
## Задача с точки зрения NER

### Постановка задачи

Необходимо обучить модель **распознавания именованных сущностей (Named Entity Recognition, NER)** для коротких и шумных пользовательских поисковых запросов.

В отличие от классических NER-задач (где сущности — это имена людей, организаций, даты и т. д.), здесь целевые сущности связаны с продуктами розничной торговли:

* **TYPE** — категория продукта,
* **BRAND** — торговая марка,
* **VOLUME** — количественные характеристики,
* **PERCENT** — проценты (например, жирность).

### Особенности данных

* Запросы содержат свободный текст с разным стилем написания.
* Присутствует **шум**: опечатки, транслитерация, сокращения, неполные слова.
* Возможны **составные сущности** («global village», «сметана простоквашино 15% 300г»).
* Сущности могут идти подряд без разделителей («печенье 200г 5шт»).
* Сущности могут встречаться несколько раз в одном запросе.

### Формализация задачи

1. Имеется входная последовательность токенов (строка поискового запроса).
2. Для каждого токена необходимо предсказать BIO-метку из множества:

   * {`B-TYPE`, `I-TYPE`, `B-BRAND`, `I-BRAND`, `B-VOLUME`, `I-VOLUME`, `B-PERCENT`, `I-PERCENT`, `O`}

3. Итоговое решение должно уметь выделять непрерывные сегменты текста и классифицировать их по типам сущностей.

### Тип модели

К задаче применимы подходы **sequence labeling**:

* **Классические модели**: CRF (Conditional Random Fields), BiLSTM+CRF.
* **Современные трансформеры**: BERT, RuBERT, DistilBERT, TinyBERT.
* Для ускорения инференса в условиях ограничения ≤1 сек оптимальны облегчённые модели:

  * `DeepPavlov/rubert-base-cased-sentence`,
  * `cointegrated/rubert-tiny2`,
  * `distilrubert`,
  * либо дообученные версии multilingual BERT.

### Подготовка данных

* Разметка предоставлена в формате BIO.
* Необходимо:

  1. Токенизировать запросы (лучше subword-токенизаторы HuggingFace).
  2. Согласовать BIO-метки с токенизацией (если слово разбилось на подслова, первой части присваивается исходная метка, остальным — `I-ENTITY`).
  3. Разделить данные на train/val для валидации.

### Обучение

* Кросс-энтропийная функция потерь (для мультиклассовой классификации меток).
* Возможность использовать CRF-слой поверх эмбеддингов для учёта зависимостей между метками.
* Аугментация данных: добавление опечаток, сокращений, случайных вставок для повышения устойчивости.

### Метрика

Используется **macro-F1 по сущностям**, где важен не только класс токена, но и точные границы сущности. Это означает, что модель должна быть особенно внимательной к началу и концу сущностей.

### Вызовы с точки зрения ML

1. **Короткие последовательности** → мало контекста для классификации.
2. **Шум в данных** (опечатки, сокращения, транслитерация). Нужна устойчивость модели к ошибкам.
3. **Дисбаланс классов**: большинство токенов — это `O`, а редкие сущности встречаются неравномерно.
4. **Скорость работы**: модель должна быть компактной (не громоздкий BERT-large).
5. **Генерализация**: брендовые наименования постоянно обновляются, поэтому модель должна уметь выделять бренды даже при отсутствии их в тренировочном наборе (обобщение по паттернам).

## Памятка по задаче NER на примере BERT

### Пример

Текст:

```
"купить молоко простоквашино"
```

Аннотация (в символах):

```
(7, 13, "B-TYPE")       → "молоко"  
(14, 26, "B-BRAND")     → "простоквашино"
```

После токенизации BERT (сабворды):

```
["[CLS]", "купить", "молоко", "прост", "##оква", "##шино", "[SEP]"]
```

BIO-разметка для сабвордов:

```
[CLS]        → O  
купить       → O  
молоко       → B-TYPE  
прост        → B-BRAND  
##оква       → I-BRAND  
##шино       → I-BRAND  
[SEP]        → O
```

### Ключевые моменты

1. **Задача похожа на мультиклассовую классификацию**

   * Каждый токен (сабворд) получает ровно один класс из множества:
     `O, B-TYPE, I-TYPE, B-BRAND, I-BRAND, B-VOLUME, I-VOLUME, B-PERCENT, I-PERCENT`.
   * Фактически это **sequence labeling**: классификация не текста целиком, а последовательности токенов.

2. **CRF-уровень**

   * Для повышения качества обычно добавляют CRF-слой поверх BERT.
   * CRF учитывает зависимости между метками (например, что `I-BRAND` не может идти без `B-BRAND`).
   * Без CRF возможны нелогичные последовательности, которые потом придётся исправлять постобработкой.

3. **Преобразование данных (input → BIO)**

   * Исходные данные даны как текст + аннотация с индексами символов.
   * При токенизации текст разбивается на сабворды.
   * Нужно через `offset_mapping` сопоставить символные индексы и токены, чтобы правильно присвоить BIO-метки.

4. **Преобразование результата (BIO → output)**

   * Модель предсказывает BIO-метку для каждого токена.
   * Эти метки нужно обратно собрать в **символьные интервалы** исходной строки.
   * Только после этого результат можно отдать в формате:

     ```
     {"start_index": 14, "end_index": 26, "entity": "B-BRAND"}
     ```
   * Это нужно для совместимости с ТЗ и дальнейшего анализа.

### Итоговое понимание

* На **уровне токенов** задача = мультиклассовая классификация.
* На **уровне последовательности** задача = sequence labeling (учёт структуры BIO).
* Нужна внимательная работа с преобразованиями:

  * текст → токены → BIO,
  * BIO → обратно в текстовые интервалы.

# Про длину последовательности

### 1. Ограничение по длине

* Мы задаём **максимальную длину последовательности**, например `max_length = 128`.
* Если текст длиннее → токенизатор обрезает (truncation).
* Если текст короче → токенизатор дополняет паддингами (`[PAD]`) до 128.

### 2. Входные данные

На вход модели подаётся:

* `input_ids` — индексы токенов (размерность = 128),
* `attention_mask` — 1 для «настоящих» токенов, 0 для паддингов (размерность = 128),
* `labels` — BIO-метки (размерность = 128).

Пример (короткий запрос):

```
input_ids     = [101, 234, 567, 890, 102, 0, 0, 0, ...]  # 128 элементов
attention_mask= [1,   1,   1,   1,   1, 0, 0, 0, ...]    # 128 элементов
labels        = [O,   B-TYPE, I-TYPE, O,   O, -100, -100, ...] # 128 элементов
```

(`-100` ставят вместо паддингов → CrossEntropy их игнорирует).

### 3. Выход модели

BERT для token classification работает так:

* Для **каждого токена** (всех 128 позиций) модель выдаёт логиты (например, размер `[128 × 9]`, если 9 классов).
* Потом берётся softmax → вероятности для каждого класса.

То есть на выходе всегда матрица `128 × N_classes`.

### 4. Что происходит с паддингами

* Для настоящих токенов (там, где `attention_mask = 1`) модель предсказывает метки.
* Для `[PAD]` (attention\_mask=0, labels=-100):

  * предсказания тоже будут, но мы их **игнорируем** (и в loss, и в инференсе).

### 5. Инференс

Когда модель делает предсказание на короткой последовательности:

* Она всё равно вернёт 128 меток (по каждому токену).
* Мы берём только первые `N` токенов (там, где `attention_mask=1`).
* Паддинги отбрасываем.

Пример:

```
Вход: "молоко простоквашино"
Токенизация: 10 сабвордов
После паддинга: 128 токенов
Выход модели: 128 меток
Берём только первые 10 (attention_mask=1), остальное игнорируем.
```

### 6. Итог

* Да, модель всегда выдаёт предсказания по всей длине (`max_length`, например 128).
* Но мы используем только часть последовательности, соответствующую реальному тексту (там, где `attention_mask=1`).
* Для обучения паддинговые токены помечаются `-100`, чтобы они не учитывались в loss.


Ниже — компактная таблица по подходящим моделям для NER, с акцентом на небольшие и русскоязычные. В столбце «Параметры» указываю только подтверждённые числа из карточек/документации; где точного числа нет — помечаю это явно.

| Модель (HF)                        |                   Параметры | Язык                     | Комментарий / зачем рассматривать                                                                                                                                                        |
| ---------------------------------- | --------------------------: | ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| cointegrated/rubert-tiny           |                        ≈12M | RU/EN                    | Очень маленькая дистилляция mBERT, быстрый инференс; хороший кандидат под ограничение ≤1 c. ([Hugging Face][1])                                                                          |
| cointegrated/rubert-tiny2          | — (точное число не указано) | RU (фокус на RU)         | Обновлённая «tiny»-версия с улучшенным словарём; часто лучше качества, чем rubert-tiny, при схожей скорости. Число параметров на карточке не раскрыто. ([Hugging Face][2])               |
| ai-forever/ruElectra-small         | — (точное число не указано) | RU                       | «Small» ELECTRA для русскоязычных данных; карточка описывает архитектуру, но не даёт точного количества параметров (классически small-ELECTRA порядка десятков млн). ([Hugging Face][3]) |
| DeepPavlov/rubert-base-cased       |                        180M | RU                       | Базовый руBERT; качественно, но тяжелее и медленнее «tiny/small» семейств. ([Hugging Face][4])                                                                                           |
| bert-base-multilingual-cased       |                        179M | Multilingual (в т.ч. RU) | Надёжный мультиязычный базлайн, но не «маленький». ([Hugging Face][5])                                                                                                                   |
| distilbert-base-multilingual-cased |                        134M | Multilingual (в т.ч. RU) | Дистилляция mBERT, быстрее и легче base-уровня; всё ещё крупнее, чем «tiny/small». ([Hugging Face][6])                                                                                   |
| xlm-roberta-base                   |                      \~125M | Multilingual (в т.ч. RU) | Часто сильный мультиязычный базлайн, но по размеру ближе к «base», не к «small». ([Hugging Face][7])                                                                                     |

Краткие рекомендации под твои требования (русский, небольшой размер, скорость):

1. Для старта и соблюдения ≤1 c: **cointegrated/rubert-tiny** или **rubert-tiny2**. Первая имеет подтверждённые \~12M параметров; вторая обычно качественнее, но карточка не публикует точный размер. ([Hugging Face][1])
2. Альтернатива с иным препроцессингом/обучением: **ruElectra-small**. ELECTRA-семейство часто даёт хорошее соотношение точности/скорости на «small», но точного числа параметров в карточке нет; ориентируйся на «малый» класс модели. ([Hugging Face][3])
3. Если нужно выжать максимум качества и есть запас по latency/ресурсам: **RuBERT base** (180M), но его придётся ускорять (оптимизация, квантизация, ONNX/TensorRT, батч-инг). ([Hugging Face][4])

Если хочешь, дополню таблицу полями «скорость на CPU/GPU», «поддержка CRF-слоя из коробки» и предложу конкретные конфигурации инференса (FP16/Int8, ONNX Runtime, динамическая обрезка по фактической длине последовательности).

[1]: https://huggingface.co/cointegrated/rubert-tiny?utm_source=chatgpt.com "cointegrated/rubert-tiny"
[2]: https://huggingface.co/cointegrated/rubert-tiny2?utm_source=chatgpt.com "cointegrated/rubert-tiny2"
[3]: https://huggingface.co/ai-forever/ruElectra-small?utm_source=chatgpt.com "ai-forever/ruElectra-small"
[4]: https://huggingface.co/DeepPavlov/rubert-base-cased?utm_source=chatgpt.com "DeepPavlov/rubert-base-cased"
[5]: https://huggingface.co/transformers/v4.9.2/pretrained_models.html?highlight=bert-base-multilingual-cased&utm_source=chatgpt.com "Pretrained models — transformers 4.7.0 documentation"
[6]: https://huggingface.co/distilbert/distilbert-base-multilingual-cased?utm_source=chatgpt.com "distilbert/distilbert-base-multilingual-cased"
[7]: https://huggingface.co/transformers/v2.4.0/pretrained_models.html?utm_source=chatgpt.com "Pretrained models — transformers 2.4.0 documentation"
