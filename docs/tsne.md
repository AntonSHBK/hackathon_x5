## ðŸ”¹ ÐšÐ°ÐºÐ¸Ðµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ Ð² NER-Ð¼Ð¾Ð´ÐµÐ»Ð¸ (BERT/DeBERTa + CRF)

1. **Token embeddings (Ð¿Ð¾ÑÐ»Ðµ encoder)**

   * Ð­Ñ‚Ð¾ Ð²Ñ‹Ñ…Ð¾Ð´ `last_hidden_state` â†’ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° `[batch, seq_len, hidden_size]`.
   * ÐšÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ñ‚Ð¾ÐºÐµÐ½Ñƒ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸ `hidden_size` (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 768).
   * Ð˜Ñ… Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð·ÑÑ‚ÑŒ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°:

     * ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°Ñ‚ÑŒ, ÐºÐ°Ðº Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ "Ñ€Ð°Ð·Ð¾ÑˆÐ»Ð¸ÑÑŒ" Ð² Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ,
     * ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ, ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·ÑƒÑŽÑ‚ÑÑ Ð»Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð²ÑÐµ Ð±Ñ€ÐµÐ½Ð´Ñ‹ Ð±Ð»Ð¸Ð¶Ðµ Ð´Ñ€ÑƒÐ³ Ðº Ð´Ñ€ÑƒÐ³Ñƒ).

2. **Entity embeddings (ÑƒÑÑ€ÐµÐ´Ð½Ñ‘Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸)**

   * ÐœÐ¾Ð¶Ð½Ð¾ Ð²Ð·ÑÑ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð°Ñ‚ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "Ð¿Ñ€Ð¾ÑÑ‚Ð¾ÐºÐ²Ð°ÑˆÐ¸Ð½Ð¾" â†’ 3 ÑÐ°Ð±Ð²Ð¾Ñ€Ð´Ð°),
   * ÑƒÑÑ€ÐµÐ´Ð½Ð¸Ñ‚ÑŒ Ð¸Ñ… ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ â†’ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð²ÐµÐºÑ‚Ð¾Ñ€ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸.
   * Ð­Ñ‚Ð¸ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ â†’ Ð¸ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ, ÐºÐ°Ðº Ñ€Ð°ÑÐ¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ñ‹ `BRAND`, `TYPE`, `PERCENT` Ð¸ Ñ‚.Ð´.

3. **\[CLS] embedding (Ð¸Ð»Ð¸ pooled output)**

   * ÐžÐ±Ñ‹Ñ‡Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð² Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸, Ð½Ð¾ Ð´Ð»Ñ NER Ð½Ðµ Ñ‚Ð°Ðº Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²ÐµÐ½.

4. **Logits (Ð´Ð¾ CRF)**

   * ÐœÐ¾Ð¶Ð½Ð¾ Ð²Ð·ÑÑ‚ÑŒ `emissions` Ð¸Ð· `classifier` (Ñ€Ð°Ð·Ð¼ÐµÑ€ `[batch, seq_len, num_labels]`).
   * Ð­Ñ‚Ð¾ Ð¿Ð¾ ÑÑƒÑ‚Ð¸ "ÐºÐ°Ðº Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¸Ð´Ð¸Ñ‚ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¼ÐµÑ‚ÐºÐ¸".
   * ÐœÐ¾Ð¶Ð½Ð¾ ÑÐ½Ð¸Ð·Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ, ÐºÐ°Ðº Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÑŽÑ‚ÑÑ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÐºÐ»Ð°ÑÑÐ¾Ð².

---

## ðŸ”¹ Ð§Ñ‚Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ

1. **Ð¡Ñ…Ð¾Ð´ÑÑ‚Ð²Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²**

   * ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, ÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ñ Ð¼ÐµÑ‚ÐºÐ¾Ð¹ `B-BRAND`, `I-BRAND`, `O` Ð¸ Ñ‚.Ð´.
   * Ð¡Ð½Ð¸Ð·Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ (UMAP / t-SNE / PCA) Ð¸ Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ scatter plot.
   * ÐœÑ‹ ÑƒÐ²Ð¸Ð´Ð¸Ð¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ "ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ" Ð¿Ð¾ Ð¼ÐµÑ‚ÐºÐ°Ð¼.

2. **Ð¡Ñ…Ð¾Ð´ÑÑ‚Ð²Ð¾ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ»Ð°ÑÑÐ°**

   * ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð±Ñ€ÐµÐ½Ð´Ñ‹ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² â†’ Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ²Ð¸Ð´ÐµÑ‚ÑŒ, Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÑŽÑ‚ Ð»Ð¸ Ð¾Ð½Ð¸ Ð¿Ð¾Ð´-ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñ‹.

3. **ÐžÑˆÐ¸Ð±ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸**

   * ÐœÐ¾Ð¶Ð½Ð¾ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð° Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ â†’ Ð¸ Ð¿Ð¾Ð½ÑÑ‚ÑŒ, Ðº ÐºÐ°ÐºÐ¸Ð¼ ÐºÐ»Ð°ÑÑÐ°Ð¼ Ð¾Ð½Ð¸ Ð±Ð»Ð¸Ð¶Ðµ.

---

## ðŸ”¹ ÐšÐ°Ðº Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ (Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ Plotly)

```python
import torch
import numpy as np
import plotly.express as px
from sklearn.decomposition import PCA

def visualize_embeddings(dataset, model, tokenizer, idx2label, max_samples=1000):
    all_embeddings = []
    all_labels = []

    loader = torch.utils.data.DataLoader(dataset, batch_size=16)
    model.eval()

    with torch.no_grad():
        for batch in loader:
            batch = {k: v.to(model.device) for k, v in batch.items()}
            outputs = model.bert(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            hidden_states = outputs.last_hidden_state.cpu().numpy()  # [batch, seq_len, hidden_size]
            labels = batch["labels"].cpu().numpy()

            for hs, lbls in zip(hidden_states, labels):
                for vec, lbl in zip(hs, lbls):
                    if lbl == -100:
                        continue
                    all_embeddings.append(vec)
                    all_labels.append(idx2label[int(lbl)])
            if len(all_embeddings) > max_samples:
                break

    X = np.array(all_embeddings[:max_samples])
    y = np.array(all_labels[:max_samples])

    # PCA â†’ 2D
    pca = PCA(n_components=2)
    X_2d = pca.fit_transform(X)

    fig = px.scatter(
        x=X_2d[:, 0], y=X_2d[:, 1],
        color=y,
        title="Token embeddings visualization",
        labels={"color": "Entity"}
    )
    fig.show()
```
