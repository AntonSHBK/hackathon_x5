# Общее описание подхода

Задача проекта заключалась в том, чтобы построить систему распознавания именованных сущностей (NER) в текстах. Основная цель — автоматически находить и классифицировать упоминания брендов, типов продуктов и других категорий в пользовательских запросах.

Для решения задачи был выстроен последовательный pipeline:

1. **Данные**
   В качестве исходного материала использовался предоставленный тренировочный датасет. Он содержал тексты и разметку в формате BIO (B-, I-, O). Для расширения покрытия и повышения устойчивости модели были сгенерированы синтетические данные:

   * увеличили разнообразие текстов,
   * добавили более длинные и сложные предложения,
   * расширили слабопредставленные классы.
     Для генерации таких данных применялись большие языковые модели (например, ChatGPT).

Пример синтетических данных:

```txt
рис националь круглозерный 900 г; [(0, 3, "B-TYPE"), (4, 14, "B-BRAND"), (28, 33, "B-VOLUME")]

гречка агроальянс 800 г; [(0, 6, "B-TYPE"), (7, 18, "B-BRAND"), (19, 24, "B-VOLUME")]

овсяные хлопья геркулес 500 г; [(0, 14, "B-TYPE"), (15, 23, "B-BRAND"), (24, 29, "B-VOLUME")]

```

2. **Предобработка и подготовка данных**

   * Тексты были токенизированы с использованием fast-токенизаторов HuggingFace.
   * BIO-разметка преобразовывалась на уровне сабвордов с помощью offset-mapping, чтобы каждая часть слова имела корректную метку.
   * Для балансировки классов в процессе подготовки данных использовались как исходные, так и синтетические примеры.

3. **Модель**
   В качестве базовой модели использовался предобученный BERT. Поверх него был переписан классификатор, чтобы добавить CRF-слой.

4. **Обучение**
   Обучение проводилось на объединённом датасете (исходные и синтетические данные). В процессе использовались стандартные техники регуляризации (Dropout, ограничение длины последовательности). Loss вычислялся через CRF.

5. **Оценка качества**
   Для оценки использовались метрики из библиотеки seqeval: precision, recall, F1 (micro и macro).

# Описание

В проекте работа над задачей велась пошагово, начиная с подготовки окружения и загрузки данных. В директории тестов находится основной блокнот **hackathon\_x5**, именно в нем был собран весь процесс обучения. В начале идут подготовительные действия, затем происходит загрузка исходного тренировочного датасета и синтетических данных. Последние использовались для расширения корпуса и повышения устойчивости модели. На этапе предобработки синтетических примеров убирались дубликаты, добавлялись необходимые метки, чтобы впоследствии корректно проводить валидацию. После этого исходные и синтетические данные объединялись в единый датасет, а на их основе формировались словари с индексами меток. Эти словари сохранялись в JSON, чтобы использовать их в дальнейшем при обучении и инференсе.

Следующий этап включал подготовку датасета к работе модели. Данные разделялись на тренировочную, тестовую и валидационную части. Для удобства и унификации этого процесса был реализован специальный класс `NerDataSet`, который брал датафрейм, выполнял токенизацию текстов, формировал attention mask, а также преобразовывал BIO-разметку в целочисленные тензоры. Таким образом, этот класс полностью готовил данные к подаче в модель.

Для обучения в качестве основы была выбрана предобученная архитектура BERT. Модель была модифицирована: из исходной реализации был убран верхний классификатор, а вместо него добавлен слой CRF. Это даёт возможность сохранить совместимость с методами HuggingFace и при этом учитывать зависимость между метками. Метод `forward` модели был переписан, чтобы обеспечить корректное взаимодействие с новым слоем.

Обучение выполнялось с настройкой ключевых параметров: задавалось количество эпох, шаги логирования, схема валидации и сохранения чекпоинтов. В качестве шедулера использовался косинусный с перезапуском. Так как в разметке наблюдается дисбаланс между классами (существует значительное количество токенов с меткой продолжения сущности и меньше — с меткой начала), была внедрена функция взвешенной потери. Это позволило уравновесить вклад разных классов при обучении и повысить качество предсказаний.

После завершения обучения итоговая модель сохранялась.
