# Модель

В качестве базовой архитектуры для задачи распознавания именованных сущностей была выбрана предобученная модель [**cointegrated/rubert-tiny2**](https://huggingface.co/cointegrated/rubert-tiny2). Эта компактная русскоязычная версия BERT хорошо подходит для работы с короткими пользовательскими запросами, характерными для ритейла.

Однако использование стандартного класса `BertForTokenClassification` из библиотеки HuggingFace оказалось недостаточным: он предсказывает метку для каждого токена независимо, что приводит к несогласованным результатам в BIO-разметке. Чтобы решить эту проблему, поверх скрытых представлений BERT был добавлен **CRF-слой (Conditional Random Field)**.

Таким образом, итоговая архитектура состоит из двух частей:

1. **BERT (rubert-tiny2)** — извлекает контекстные представления токенов из текста;
2. **CRF-слой** — моделирует зависимости между метками последовательности и обеспечивает согласованное предсказание цепочек сущностей.

После прохождения текста через BERT выходные эмбеддинги преобразуются линейным слоем в логиты (emissions), которые затем поступают в CRF. В процессе обучения CRF использует функцию потерь, учитывающую правильность всей последовательности, а в режиме инференса применяется метод `decode`, возвращающий наиболее вероятную разметку.

Преимущества:

* учитываются переходы между метками (например, после `B-TYPE` корректно предсказывается `I-TYPE`, а не `B-BRAND`);
* повышается качество распознавания последовательностей в BIO-формате;
* снижается количество "разрывов" сущностей и случайных ошибок на коротких запросах.

