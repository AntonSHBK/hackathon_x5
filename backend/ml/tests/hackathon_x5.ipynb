{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce5d94a",
   "metadata": {},
   "source": [
    "# X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ebf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def seed_all(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('../../../data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_DOWNLOAD = DATA_PATH / Path('download/')\n",
    "DATA_PATH_DOWNLOAD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_DATASET = DATA_PATH / Path('datasets/')\n",
    "DATA_PATH_DATASET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETICS = DATA_PATH / Path('synthetics/')\n",
    "DATA_PATH_DATASET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = DATA_PATH / Path('cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = DATA_PATH / Path('models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_IMGS = DATA_PATH /  Path('imgs/')\n",
    "DATA_IMGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "# BASE_MODEL_NAME = 'DeepPavlov/rubert-base-cased'\n",
    "MODEL_NAME_SAVE = \"ner_x5\"\n",
    "MODEL_CHECKPOINT_PATH = \"ner_x5_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66775d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "DATA_LOG = DATA_PATH / Path(f'../log/{MODEL_NAME_SAVE}_{current_date}')\n",
    "DATA_LOG.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008b155",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db66fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5 = pd.read_csv(DATA_PATH_DATASET / \"train.csv\", sep=\";\")\n",
    "\n",
    "df_x5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19774fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_annotations(ann_list):\n",
    "    if isinstance(ann_list, str):\n",
    "        ann_list = eval(ann_list)\n",
    "    new_list = []\n",
    "    for start, end, label in ann_list:\n",
    "        if label == \"0\":\n",
    "            label = \"O\"\n",
    "        new_list.append((start, end, label))\n",
    "    return new_list\n",
    "\n",
    "df_x5[\"annotation\"] = df_x5[\"annotation\"].apply(normalize_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0514259",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "\n",
    "for ann_list in df_x5[\"annotation\"]:\n",
    "    if isinstance(ann_list, str):\n",
    "        ann_list = eval(ann_list)\n",
    "    for _, _, label in ann_list:\n",
    "        all_labels.add(label)\n",
    "\n",
    "unique_labels = sorted(all_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c13579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c18957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "def load_synthetic_file(file_path: str | Path) -> pd.DataFrame:\n",
    "    file_path = Path(file_path)\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                text, ann_str = line.split(\";\", 1)\n",
    "                text = text.strip()\n",
    "                annotation = ast.literal_eval(ann_str.strip())\n",
    "                data.append({\"sample\": text, \"annotation\": annotation})\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке строки в {file_path}: {line}\\n{e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_dataset(data_dir: str | Path) -> pd.DataFrame:\n",
    "    data_dir = Path(data_dir)\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in data_dir.glob(\"*.txt\"):\n",
    "        df_file = load_synthetic_file(file_path)\n",
    "        all_dfs.append(df_file)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"sample\", \"annotation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6838138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic = load_synthetic_dataset(DATA_SYNTHETICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce666f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5[\"df_label\"] = \"x5\"\n",
    "df_synthetic[\"df_label\"] = \"synthetic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_synthetic, df_x5], ignore_index=True)\n",
    "df_train = df_train.drop_duplicates(subset=[\"sample\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e3993",
   "metadata": {},
   "source": [
    "# Словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c21ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = {label for anns in df_train[\"annotation\"] for _, _, label in anns if label != \"O\"}\n",
    "\n",
    "b_labels = sorted([lbl for lbl in unique_labels if lbl.startswith(\"B-\")])\n",
    "i_labels = {lbl[2:]: lbl for lbl in unique_labels if lbl.startswith(\"I-\")}\n",
    "\n",
    "all_labels = [\"O\"]\n",
    "for b in b_labels:\n",
    "    all_labels.append(b)\n",
    "    base = b[2:]\n",
    "    if base in i_labels:\n",
    "        all_labels.append(i_labels[base])\n",
    "\n",
    "label2idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "print(\"label2idx:\", label2idx)\n",
    "print(\"idx2label:\", idx2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c72237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "label2idx_path = DATA_PATH_SAVE_MODELS / \"label2idx.json\"\n",
    "idx2label_path = DATA_PATH_SAVE_MODELS / \"idx2label.json\"\n",
    "\n",
    "with open(label2idx_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(idx2label_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2label, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Словарь label2idx сохранён в {label2idx_path}\")\n",
    "print(f\"Словарь idx2label сохранён в {idx2label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24937c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad2c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d56d0c1a",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc707e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data: pd.DataFrame\n",
    "test_data: pd.DataFrame\n",
    "train_data, test_data = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "val_data: pd.DataFrame\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edea836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "\n",
    "class NerDataSet(Dataset):\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, \n",
    "        max_length: int, \n",
    "        tokenizer_path: str, \n",
    "        label2idx: Dict[str, int],\n",
    "        cache_dir: str = None, \n",
    "        text_label: str = 'sample',\n",
    "        target_label: str = 'annotation',        \n",
    "        dtype_input_ids: torch.dtype = torch.long,\n",
    "        dtype_token_type_ids: torch.dtype = torch.long,\n",
    "        dtype_attention_mask: torch.dtype = torch.long,\n",
    "        dtype_labels : torch.dtype = torch.long,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.max_length = max_length\n",
    "        self.text_label = text_label\n",
    "        self.target_label = target_label\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.label2idx = label2idx\n",
    "        \n",
    "        # TODO добавить класс для типизации\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_path,\n",
    "            cache_dir=cache_dir,\n",
    "            use_fast=True,\n",
    "        )\n",
    "\n",
    "        self.dtype_input_ids = dtype_input_ids\n",
    "        self.dtype_token_type_ids = dtype_token_type_ids\n",
    "        self.dtype_attention_mask = dtype_attention_mask\n",
    "        self.dtype_labels  = dtype_labels \n",
    "\n",
    "        self.input_ids, self.token_type_ids, self.attention_mask, self.labels = self.tokenize_data()\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        input_ids, token_type_ids, attention_mask, labels = [], [], [], []\n",
    "        tokens_ids_debug, tokens_text_debug, labels_debug = [], [], []\n",
    "\n",
    "        for _, row in tqdm(\n",
    "            self.df.iterrows(),\n",
    "            total=len(self.df),\n",
    "            desc=\"Tokenizing data\",\n",
    "            ncols=100\n",
    "        ):\n",
    "            text = row[self.text_label]\n",
    "            ann_list = row[self.target_label]\n",
    "\n",
    "            if isinstance(ann_list, str):\n",
    "                ann_list = eval(ann_list)\n",
    "\n",
    "            encoded = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_offsets_mapping=True,\n",
    "                return_token_type_ids=True,\n",
    "            )\n",
    "\n",
    "            offsets = encoded[\"offset_mapping\"]\n",
    "            seq_labels = [\"O\"] * len(offsets)\n",
    "\n",
    "            for start, end, ent_label in ann_list:\n",
    "                inside = False\n",
    "                for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                    if tok_start >= end:\n",
    "                        break\n",
    "                    if tok_end <= start:\n",
    "                        continue\n",
    "\n",
    "                    if not inside:\n",
    "                        seq_labels[i] = ent_label  # B-XXX\n",
    "                        inside = True\n",
    "                    else:\n",
    "                        # преобразуем \"B-XXX\" → \"I-XXX\"\n",
    "                        if ent_label.startswith(\"B-\"):\n",
    "                            seq_labels[i] = \"I-\" + ent_label.split(\"-\", 1)[1]\n",
    "                        else:\n",
    "                            seq_labels[i] = ent_label\n",
    "\n",
    "            # конвертация в индексы\n",
    "            label_ids = []\n",
    "            for i, label in enumerate(seq_labels):\n",
    "                if encoded[\"attention_mask\"][i] == 0:\n",
    "                    label_ids.append(-100)\n",
    "                else:\n",
    "                    label_ids.append(self.label2idx.get(label, self.label2idx[\"O\"]))\n",
    "\n",
    "            # добавляем в массивы\n",
    "            input_ids.append(torch.tensor(encoded[\"input_ids\"], dtype=self.dtype_input_ids))\n",
    "            token_type_ids.append(torch.tensor(encoded.get(\"token_type_ids\", [0]*len(label_ids)), dtype=self.dtype_token_type_ids))\n",
    "            attention_mask.append(torch.tensor(encoded[\"attention_mask\"], dtype=self.dtype_attention_mask))\n",
    "            labels.append(torch.tensor(label_ids, dtype=self.dtype_labels))\n",
    "\n",
    "            if self.debug:\n",
    "                tokens_ids_debug.append(encoded[\"input_ids\"])\n",
    "                tokens_text_debug.append(self.tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"]))\n",
    "                labels_debug.append(seq_labels)\n",
    "\n",
    "        input_ids = torch.stack(input_ids)\n",
    "        token_type_ids = torch.stack(token_type_ids)\n",
    "        attention_mask = torch.stack(attention_mask)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        if self.debug:\n",
    "            self.df[\"tokens_ids_debug\"] = tokens_ids_debug\n",
    "            self.df[\"tokens_text_debug\"] = tokens_text_debug\n",
    "            self.df[\"labels_debug\"] = labels_debug\n",
    "\n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"token_type_ids\": self.token_type_ids[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "        \n",
    "    def plot_token_length_distribution(self):\n",
    "        \"\"\"\n",
    "        Строит гистограмму распределения длины токенов (без специальных токенов и паддинга).\n",
    "        Работает только если класс инициализирован с debug=True.\n",
    "        \"\"\"\n",
    "        if not self.debug:\n",
    "            raise ValueError(\"Для построения графика необходимо включить debug=True при инициализации.\")\n",
    "\n",
    "        token_lengths = []\n",
    "        special_ids = set(self.tokenizer.all_special_ids)\n",
    "\n",
    "        for token_ids in self.df[\"tokens_ids_debug\"]:\n",
    "            filtered_tokens = [tid for tid in token_ids if tid not in special_ids]\n",
    "            token_lengths.append(len(filtered_tokens))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(token_lengths, bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "        plt.xlabel(\"Длина текста (количество токенов)\")\n",
    "        plt.ylabel(\"Частота\")\n",
    "        plt.title(\"Распределение длин текстов в токенах\")\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.show()\n",
    "        \n",
    "    @staticmethod\n",
    "    def prepare_text(text: str, tokenizer, max_length: int = 128, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Подготавливает текст для подачи в модель.\n",
    "        Возвращает тензоры (input_ids, attention_mask, token_type_ids, offset_mapping).\n",
    "        \"\"\"\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # переносим на устройство\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        return encoded\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_predictions(\n",
    "        text: str,\n",
    "        predictions: torch.Tensor,\n",
    "        tokenizer,\n",
    "        idx2label: Dict[int, str],\n",
    "        encoded_inputs: Dict[str, torch.Tensor]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Преобразует выход модели (predictions) в список сущностей с индексами.\n",
    "        Args:\n",
    "            text (str): исходный текст\n",
    "            predictions (torch.Tensor): предсказанные индексы меток (shape [seq_len])\n",
    "            tokenizer: токенизатор\n",
    "            idx2label: словарь {id: label}\n",
    "            encoded_inputs: результат prepare_text (для offset_mapping)\n",
    "        Returns:\n",
    "            List[Dict]: список сущностей {start, end, label, entity}\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[\"input_ids\"][0])\n",
    "        offsets = encoded_inputs[\"offset_mapping\"][0].cpu().numpy()\n",
    "        labels = [idx2label.get(int(p), \"O\") for p in predictions]\n",
    "\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "\n",
    "        for token, (start, end), label in zip(tokens, offsets, labels):\n",
    "            if label == \"O\" or token in tokenizer.all_special_tokens:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "                continue\n",
    "\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = {\"start\": int(start), \"end\": int(end), \"label\": label[2:], \"entity\": text[start:end]}\n",
    "            elif label.startswith(\"I-\") and current_entity and current_entity[\"label\"] == label[2:]:\n",
    "                current_entity[\"end\"] = int(end)\n",
    "                current_entity[\"entity\"] = text[current_entity[\"start\"]:end]\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = {\"start\": int(start), \"end\": int(end), \"label\": label[2:], \"entity\": text[start:end]}\n",
    "\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "\n",
    "        return entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_input = torch.long\n",
    "dtype_labels = torch.long\n",
    "\n",
    "train_dataset = NerDataSet(\n",
    "    df=train_data, \n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True    \n",
    ")\n",
    "\n",
    "val_dataset = NerDataSet(\n",
    "    df=val_data,\n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "test_dataset = NerDataSet(\n",
    "    df=test_data, \n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ff487",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d39e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.plot_token_length_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5b27d",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers.utils import ModelOutput\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenClassifierCRFOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Выход модели для NER с CRF.\n",
    "    \"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None  # [batch, seq_len, num_labels]\n",
    "    predictions: Optional[torch.LongTensor] = None  # [batch, seq_len] с паддингами (-100)\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "\n",
    "\n",
    "class BertForTokenClassificationCRF(BertForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> TokenClassifierCRFOutput:\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]   # [batch, seq_len, hidden]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)  # [batch, seq_len, num_labels]\n",
    "\n",
    "        loss, predictions = None, None\n",
    "        if labels is not None:\n",
    "            labels_for_crf = labels.clone()\n",
    "            labels_for_crf[labels_for_crf == -100] = 0\n",
    "\n",
    "            loss = -self.crf(\n",
    "                emissions,\n",
    "                labels_for_crf,\n",
    "                mask=attention_mask.bool(),\n",
    "                reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "        decoded = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "\n",
    "        max_len = emissions.size(1)\n",
    "        predictions_padded = torch.full(\n",
    "            (len(decoded), max_len),\n",
    "            fill_value=-100,\n",
    "            dtype=torch.long,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "        for i, seq in enumerate(decoded):\n",
    "            predictions_padded[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=emissions.device)\n",
    "\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (emissions,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierCRFOutput(\n",
    "            loss=loss,\n",
    "            logits=emissions,\n",
    "            predictions=predictions_padded,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9dc2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "num_labels = len(label2idx)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=idx2label,     # словарь {int: str}\n",
    "    label2id=label2idx,     # словарь {str: int}\n",
    "    cache_dir=DATA_CACHE,\n",
    ")\n",
    "\n",
    "model = BertForTokenClassificationCRF.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236b230",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7178331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Метрики для NER (BIO-разметка) с CRF.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        seq_true = []\n",
    "        seq_pred = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            seq_true.append(idx2label[l])\n",
    "            seq_pred.append(idx2label[p])\n",
    "        true_labels.append(seq_true)\n",
    "        true_predictions.append(seq_pred)\n",
    "\n",
    "    # основные метрики\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n",
    "    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, digits=4)\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"report\": report,\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115feb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomBaseTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Кастомный Trainer, наследуемый от transformers.Trainer.\n",
    "    https://hf.qhduan.com/docs/transformers/trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, args, **kwargs):\n",
    "        super().__init__(model, args, **kwargs)\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"\n",
    "        Графики потерь и метрик на основе `trainer.state.log_history`.\n",
    "\n",
    "        Графики строятся для:\n",
    "        - `loss` (потери на обучении)\n",
    "        - `eval_loss` (потери на валидации)\n",
    "        - `eval_accuracy`, `eval_f1`, `eval_f1_macro` (если они были логированы)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.state.log_history:\n",
    "            print(\"Нет данных для построения графиков. Проверьте, выполнялось ли обучение.\")\n",
    "            return\n",
    "        \n",
    "        # Загружаем историю логов в DataFrame\n",
    "        log_data = pd.DataFrame(self.state.log_history)\n",
    "\n",
    "        # Фильтруем только строки с эпохами\n",
    "        log_data = log_data.dropna(subset=[\"epoch\"])  # Оставляем только строки с эпохами\n",
    "        log_data = log_data.groupby(\"epoch\").last().reset_index()  # Убираем дубли по эпохам\n",
    "        \n",
    "        # Список метрик, которые можно отобразить\n",
    "        available_metrics = [col for col in log_data.columns if col.startswith(\"eval_\") or col == \"loss\"]\n",
    "\n",
    "        # Определяем количество графиков\n",
    "        num_plots = len(available_metrics)\n",
    "        plt.figure(figsize=(8, 4 * num_plots))\n",
    "\n",
    "        for i, metric in enumerate(available_metrics, start=1):\n",
    "            plt.subplot(num_plots, 1, i)\n",
    "            plt.plot(log_data[\"epoch\"], log_data[metric], marker=\"o\", label=metric)\n",
    "\n",
    "            plt.xlabel(\"Эпохи\")\n",
    "            plt.ylabel(metric)\n",
    "            plt.title(f\"График {metric}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec50deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_PATH_SAVE_MODELS / MODEL_CHECKPOINT_PATH,  # Папка для сохранения моделей\n",
    "    num_train_epochs=10,  # Количество эпох\n",
    "\n",
    "    eval_strategy=\"steps\",  # Оценка модели после каждой эпохи\n",
    "    eval_steps=200,  # Оценка модели каждый шаг\n",
    "    logging_strategy=\"steps\",  # Логирование каждые N шагов\n",
    "    logging_steps=200,  # Как часто логировать\n",
    "    disable_tqdm=False,  # Отключить tqdm (нужно для работы в Colab/Kaggle)\n",
    "    report_to=\"tensorboard\",  # Логируем в TensorBoard\n",
    "    logging_dir=DATA_LOG,  # Папка для логов\n",
    "\n",
    "    save_strategy=\"steps\",  # Сохранение модели после каждой эпохи\n",
    "    save_steps=200, # Если save_strategy=\"steps\"\n",
    "    save_total_limit=5,  # Храним все\n",
    "    load_best_model_at_end=True,  # Загружать лучшую модель после обучения\n",
    "    metric_for_best_model=\"eval_f1_macro\",  # Выбираем лучшую модель по eval_f1_macro\n",
    "    greater_is_better=True,  # Чем меньше eval_loss, тем лучше модель\n",
    "\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Размер батча на одно устройство (GPU/CPU)\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Размер батча для валидации\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "\n",
    "steps_per_epoch = len(train_dataset.df) // training_args.per_device_train_batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.2 * total_steps),    # 20% шагов на разогрев\n",
    "    num_training_steps=total_steps              # полный цикл косинусного затухания\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomBaseTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=train_dataset.tokenizer,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33452675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25824222",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.predict(test_dataset)\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b28b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(DATA_PATH_SAVE_MODELS / MODEL_NAME_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ea372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "class NERPipelineCRF:\n",
    "    def __init__(self, model_path: str, label2idx: dict, idx2label: dict, device: str = None, max_length: int = 128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_path (str): Путь к сохранённой модели (чекпоинт HuggingFace).\n",
    "            label2idx (dict): Словарь {label: idx}.\n",
    "            idx2label (dict): Словарь {idx: label}.\n",
    "            device (str): \"cuda\" или \"cpu\". Если None -> auto.\n",
    "            max_length (int): Максимальная длина текста при токенизации.\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.label2idx = label2idx\n",
    "        self.idx2label = idx2label\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        config = AutoConfig.from_pretrained(model_path)\n",
    "        config.label2id = label2idx\n",
    "        config.id2label = idx2label\n",
    "\n",
    "        self.model = BertForTokenClassificationCRF.from_pretrained(model_path, config=config)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict_text(self, text: str):\n",
    "        \"\"\"\n",
    "        Предсказание сущностей для одного текста.\n",
    "        Возвращает список словарей {start, end, label, entity}.\n",
    "        \"\"\"\n",
    "        encoded = NerDataSet.prepare_text(text, self.tokenizer, max_length=self.max_length, device=self.device)\n",
    "\n",
    "        offset_mapping = encoded.pop(\"offset_mapping\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded)\n",
    "            preds = outputs.predictions[0]  # [seq_len]\n",
    "\n",
    "        encoded[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        entities = NerDataSet.decode_predictions(\n",
    "            text,\n",
    "            preds,\n",
    "            self.tokenizer,\n",
    "            self.idx2label,\n",
    "            encoded\n",
    "        )\n",
    "        return entities\n",
    "\n",
    "\n",
    "    def predict_dataset(self, dataset, batch_size: int = 16):\n",
    "        \"\"\"\n",
    "        Предсказания для всего датасета.\n",
    "        Возвращает список списков сущностей (по строкам датасета).\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                preds = outputs.predictions.cpu().numpy()\n",
    "\n",
    "                for i, pred_seq in enumerate(preds):\n",
    "                    text = dataset.df.iloc[len(all_results) + i][dataset.text_label]\n",
    "                    encoded = NerDataSet.prepare_text(text, self.tokenizer, max_length=self.max_length, device=\"cpu\")\n",
    "                    entities = NerDataSet.decode_predictions(text, pred_seq, self.tokenizer, self.idx2label, encoded)\n",
    "                    all_results.append(entities)\n",
    "\n",
    "        return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a303d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = DATA_PATH_SAVE_MODELS / MODEL_NAME_SAVE\n",
    "label2idx_path = DATA_PATH_SAVE_MODELS / \"label2idx.json\"\n",
    "idx2label_path = DATA_PATH_SAVE_MODELS / \"idx2label.json\"\n",
    "\n",
    "with open(label2idx_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "\n",
    "with open(idx2label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    idx2label = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "pipeline = NERPipelineCRF(\n",
    "    model_path=model_dir,\n",
    "    label2idx=label2idx,\n",
    "    idx2label=idx2label,\n",
    "    max_length=16\n",
    ")\n",
    "\n",
    "text = \"купить молоко простоквашино что и как\"\n",
    "entities = pipeline.predict_text(text)\n",
    "print(entities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
