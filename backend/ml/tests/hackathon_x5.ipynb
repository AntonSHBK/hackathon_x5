{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce5d94a",
   "metadata": {},
   "source": [
    "# X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ebf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def seed_all(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('../../../data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_DOWNLOAD = DATA_PATH / Path('download/')\n",
    "DATA_PATH_DOWNLOAD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_DATASET = DATA_PATH / Path('datasets/')\n",
    "DATA_PATH_DATASET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = DATA_PATH / Path('cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = DATA_PATH / Path('models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_IMGS = DATA_PATH /  Path('imgs/')\n",
    "DATA_IMGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = 'cointegrated/rubert-tiny2'\n",
    "MODEL_NAME_SAVE = \"ner_x5\"\n",
    "MODEL_CHECKPOINT_PATH = \"ner_x5_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66775d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "DATA_LOG = DATA_PATH / Path(f'../log/{MODEL_NAME_SAVE}_{current_date}')\n",
    "DATA_LOG.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008b155",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db66fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5 = pd.read_csv(DATA_PATH_DATASET / \"train.csv\", sep=\";\")\n",
    "\n",
    "df_x5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19774fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_annotations(ann_list):\n",
    "    if isinstance(ann_list, str):\n",
    "        ann_list = eval(ann_list)\n",
    "    new_list = []\n",
    "    for start, end, label in ann_list:\n",
    "        if label == \"0\":\n",
    "            label = \"O\"\n",
    "        new_list.append((start, end, label))\n",
    "    return new_list\n",
    "\n",
    "df_x5[\"annotation\"] = df_x5[\"annotation\"].apply(normalize_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0514259",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "\n",
    "for ann_list in df_x5[\"annotation\"]:\n",
    "    if isinstance(ann_list, str):\n",
    "        ann_list = eval(ann_list)\n",
    "    for _, _, label in ann_list:\n",
    "        all_labels.add(label)\n",
    "\n",
    "unique_labels = sorted(all_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c13579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_x5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f7180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "362e3993",
   "metadata": {},
   "source": [
    "# Словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c21ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = {label for anns in df_train[\"annotation\"] for _, _, label in anns if label != \"O\"}\n",
    "\n",
    "b_labels = sorted([lbl for lbl in unique_labels if lbl.startswith(\"B-\")])\n",
    "i_labels = {lbl[2:]: lbl for lbl in unique_labels if lbl.startswith(\"I-\")}\n",
    "\n",
    "all_labels = [\"O\"]\n",
    "for b in b_labels:\n",
    "    all_labels.append(b)\n",
    "    base = b[2:]\n",
    "    if base in i_labels:\n",
    "        all_labels.append(i_labels[base])\n",
    "\n",
    "label2idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "print(\"label2idx:\", label2idx)\n",
    "print(\"idx2label:\", idx2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c72237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "label2idx_path = DATA_PATH_SAVE_MODELS / \"label2idx.json\"\n",
    "idx2label_path = DATA_PATH_SAVE_MODELS / \"idx2label.json\"\n",
    "\n",
    "with open(label2idx_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(idx2label_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2label, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Словарь label2idx сохранён в {label2idx_path}\")\n",
    "print(f\"Словарь idx2label сохранён в {idx2label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24937c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad2c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d56d0c1a",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc707e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data: pd.DataFrame\n",
    "test_data: pd.DataFrame\n",
    "train_data, test_data = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "val_data: pd.DataFrame\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edea836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "\n",
    "class NerDataSet(Dataset):\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, \n",
    "        max_length: int, \n",
    "        tokenizer_path: str, \n",
    "        label2idx: Dict[str, int],\n",
    "        cache_dir: str = None, \n",
    "        text_label: str = 'sample',\n",
    "        target_label: str = 'annotation',        \n",
    "        dtype_input_ids: torch.dtype = torch.long,\n",
    "        dtype_token_type_ids: torch.dtype = torch.long,\n",
    "        dtype_attention_mask: torch.dtype = torch.long,\n",
    "        dtype_labels : torch.dtype = torch.long,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.df = df.copy().reset_index(drop=True)\n",
    "        self.max_length = max_length\n",
    "        self.text_label = text_label\n",
    "        self.target_label = target_label\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.label2idx = label2idx\n",
    "        \n",
    "        # TODO добавить класс для типизации\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_path,\n",
    "            cache_dir=cache_dir,\n",
    "            use_fast=True,\n",
    "        )\n",
    "\n",
    "        self.dtype_input_ids = dtype_input_ids\n",
    "        self.dtype_token_type_ids = dtype_token_type_ids\n",
    "        self.dtype_attention_mask = dtype_attention_mask\n",
    "        self.dtype_labels  = dtype_labels \n",
    "\n",
    "        self.input_ids, self.token_type_ids, self.attention_mask, self.labels = self.tokenize_data()\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        input_ids, token_type_ids, attention_mask, labels = [], [], [], []\n",
    "        tokens_ids_debug, tokens_text_debug, labels_debug = [], [], []\n",
    "\n",
    "        for _, row in tqdm(\n",
    "            self.df.iterrows(),\n",
    "            total=len(self.df),\n",
    "            desc=\"Tokenizing data\",\n",
    "            ncols=100\n",
    "        ):\n",
    "            text = row[self.text_label]\n",
    "            ann_list = row[self.target_label]\n",
    "\n",
    "            if isinstance(ann_list, str):\n",
    "                ann_list = eval(ann_list)\n",
    "\n",
    "            encoded = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_offsets_mapping=True,\n",
    "                return_token_type_ids=True,\n",
    "            )\n",
    "\n",
    "            offsets = encoded[\"offset_mapping\"]\n",
    "            seq_labels = [\"O\"] * len(offsets)\n",
    "\n",
    "            for start, end, ent_label in ann_list:\n",
    "                inside = False\n",
    "                for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                    if tok_start >= end:\n",
    "                        break\n",
    "                    if tok_end <= start:\n",
    "                        continue\n",
    "\n",
    "                    if not inside:\n",
    "                        seq_labels[i] = ent_label  # B-XXX\n",
    "                        inside = True\n",
    "                    else:\n",
    "                        # преобразуем \"B-XXX\" → \"I-XXX\"\n",
    "                        if ent_label.startswith(\"B-\"):\n",
    "                            seq_labels[i] = \"I-\" + ent_label.split(\"-\", 1)[1]\n",
    "                        else:\n",
    "                            seq_labels[i] = ent_label\n",
    "\n",
    "            # конвертация в индексы\n",
    "            label_ids = []\n",
    "            for i, label in enumerate(seq_labels):\n",
    "                if encoded[\"attention_mask\"][i] == 0:\n",
    "                    label_ids.append(-100)\n",
    "                else:\n",
    "                    label_ids.append(self.label2idx.get(label, self.label2idx[\"O\"]))\n",
    "\n",
    "            # добавляем в массивы\n",
    "            input_ids.append(torch.tensor(encoded[\"input_ids\"], dtype=self.dtype_input_ids))\n",
    "            token_type_ids.append(torch.tensor(encoded.get(\"token_type_ids\", [0]*len(label_ids)), dtype=self.dtype_token_type_ids))\n",
    "            attention_mask.append(torch.tensor(encoded[\"attention_mask\"], dtype=self.dtype_attention_mask))\n",
    "            labels.append(torch.tensor(label_ids, dtype=self.dtype_labels))\n",
    "\n",
    "            if self.debug:\n",
    "                tokens_ids_debug.append(encoded[\"input_ids\"])\n",
    "                tokens_text_debug.append(self.tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"]))\n",
    "                labels_debug.append(seq_labels)\n",
    "\n",
    "        input_ids = torch.stack(input_ids)\n",
    "        token_type_ids = torch.stack(token_type_ids)\n",
    "        attention_mask = torch.stack(attention_mask)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        if self.debug:\n",
    "            self.df[\"tokens_ids_debug\"] = tokens_ids_debug\n",
    "            self.df[\"tokens_text_debug\"] = tokens_text_debug\n",
    "            self.df[\"labels_debug\"] = labels_debug\n",
    "\n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"token_type_ids\": self.token_type_ids[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "        \n",
    "    def plot_token_length_distribution(self):\n",
    "        \"\"\"\n",
    "        Строит гистограмму распределения длины токенов (без специальных токенов и паддинга).\n",
    "        Работает только если класс инициализирован с debug=True.\n",
    "        \"\"\"\n",
    "        if not self.debug:\n",
    "            raise ValueError(\"Для построения графика необходимо включить debug=True при инициализации.\")\n",
    "\n",
    "        token_lengths = []\n",
    "        special_ids = set(self.tokenizer.all_special_ids)\n",
    "\n",
    "        for token_ids in self.df[\"tokens_ids_debug\"]:\n",
    "            filtered_tokens = [tid for tid in token_ids if tid not in special_ids]\n",
    "            token_lengths.append(len(filtered_tokens))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(token_lengths, bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "        plt.xlabel(\"Длина текста (количество токенов)\")\n",
    "        plt.ylabel(\"Частота\")\n",
    "        plt.title(\"Распределение длин текстов в токенах\")\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_input = torch.long\n",
    "dtype_labels = torch.long\n",
    "\n",
    "train_dataset = NerDataSet(\n",
    "    df=train_data, \n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True    \n",
    ")\n",
    "\n",
    "val_dataset = NerDataSet(\n",
    "    df=val_data,\n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "test_dataset = NerDataSet(\n",
    "    df=test_data, \n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ff487",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d39e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.plot_token_length_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5b27d",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers.utils import ModelOutput\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenClassifierCRFOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Выход модели для NER с CRF.\n",
    "    \"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None  # [batch, seq_len, num_labels]\n",
    "    predictions: Optional[torch.LongTensor] = None  # [batch, seq_len] с паддингами (-100)\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "\n",
    "\n",
    "class BertForTokenClassificationCRF(BertForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> TokenClassifierCRFOutput:\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]   # [batch, seq_len, hidden]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)  # [batch, seq_len, num_labels]\n",
    "\n",
    "        loss, predictions = None, None\n",
    "        if labels is not None:\n",
    "            labels_for_crf = labels.clone()\n",
    "            labels_for_crf[labels_for_crf == -100] = 0\n",
    "\n",
    "            loss = -self.crf(\n",
    "                emissions,\n",
    "                labels_for_crf,\n",
    "                mask=attention_mask.bool(),\n",
    "                reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "        decoded = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "\n",
    "        max_len = emissions.size(1)\n",
    "        predictions_padded = torch.full(\n",
    "            (len(decoded), max_len),\n",
    "            fill_value=-100,\n",
    "            dtype=torch.long,\n",
    "            device=emissions.device,\n",
    "        )\n",
    "        for i, seq in enumerate(decoded):\n",
    "            predictions_padded[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=emissions.device)\n",
    "\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (emissions,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierCRFOutput(\n",
    "            loss=loss,\n",
    "            logits=emissions,\n",
    "            predictions=predictions_padded,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9dc2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "num_labels = len(label2idx)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=idx2label,     # словарь {int: str}\n",
    "    label2id=label2idx,     # словарь {str: int}\n",
    "    cache_dir=DATA_CACHE,\n",
    ")\n",
    "\n",
    "model = BertForTokenClassificationCRF.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236b230",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7178331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Метрики для NER (BIO-разметка) с CRF.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        seq_true = []\n",
    "        seq_pred = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            seq_true.append(idx2label[l])\n",
    "            seq_pred.append(idx2label[p])\n",
    "        true_labels.append(seq_true)\n",
    "        true_predictions.append(seq_pred)\n",
    "\n",
    "    # основные метрики\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n",
    "    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, digits=4)\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"report\": report,\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115feb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomBaseTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Кастомный Trainer, наследуемый от transformers.Trainer.\n",
    "    https://hf.qhduan.com/docs/transformers/trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, args, **kwargs):\n",
    "        super().__init__(model, args, **kwargs)\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"\n",
    "        Графики потерь и метрик на основе `trainer.state.log_history`.\n",
    "\n",
    "        Графики строятся для:\n",
    "        - `loss` (потери на обучении)\n",
    "        - `eval_loss` (потери на валидации)\n",
    "        - `eval_accuracy`, `eval_f1`, `eval_f1_macro` (если они были логированы)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.state.log_history:\n",
    "            print(\"Нет данных для построения графиков. Проверьте, выполнялось ли обучение.\")\n",
    "            return\n",
    "        \n",
    "        # Загружаем историю логов в DataFrame\n",
    "        log_data = pd.DataFrame(self.state.log_history)\n",
    "\n",
    "        # Фильтруем только строки с эпохами\n",
    "        log_data = log_data.dropna(subset=[\"epoch\"])  # Оставляем только строки с эпохами\n",
    "        log_data = log_data.groupby(\"epoch\").last().reset_index()  # Убираем дубли по эпохам\n",
    "        \n",
    "        # Список метрик, которые можно отобразить\n",
    "        available_metrics = [col for col in log_data.columns if col.startswith(\"eval_\") or col == \"loss\"]\n",
    "\n",
    "        # Определяем количество графиков\n",
    "        num_plots = len(available_metrics)\n",
    "        plt.figure(figsize=(8, 4 * num_plots))\n",
    "\n",
    "        for i, metric in enumerate(available_metrics, start=1):\n",
    "            plt.subplot(num_plots, 1, i)\n",
    "            plt.plot(log_data[\"epoch\"], log_data[metric], marker=\"o\", label=metric)\n",
    "\n",
    "            plt.xlabel(\"Эпохи\")\n",
    "            plt.ylabel(metric)\n",
    "            plt.title(f\"График {metric}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec50deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # 🟢 Общие параметры тренировки\n",
    "    output_dir=DATA_PATH_SAVE_MODELS / MODEL_CHECKPOINT_PATH,  # Папка для сохранения моделей\n",
    "    # learning_rate=1e-4,  # Скорость обучения\n",
    "    num_train_epochs=10,  # Количество эпох\n",
    "    # weight_decay=1e-2,  # L2-регуляризация\n",
    "    # optim=\"adamw_torch\",  # Оптимизатор AdamW\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # warmup_ratio=0.1,\n",
    "\n",
    "    # 🔵 Оценка и логирование\n",
    "    eval_strategy=\"steps\",  # Оценка модели после каждой эпохи\n",
    "    eval_steps=100,  # Оценка модели каждый шаг\n",
    "    logging_strategy=\"steps\",  # Логирование каждые N шагов\n",
    "    logging_steps=100,  # Как часто логировать\n",
    "    disable_tqdm=False,  # Отключить tqdm (нужно для работы в Colab/Kaggle)\n",
    "    report_to=\"tensorboard\",  # Логируем в TensorBoard\n",
    "    logging_dir=DATA_LOG,  # Папка для логов\n",
    "\n",
    "    # 🟠 Сохранение моделей\n",
    "    save_strategy=\"steps\",  # Сохранение модели после каждой эпохи\n",
    "    save_steps=100, # Если save_strategy=\"steps\"\n",
    "    save_total_limit=5,  # Храним все\n",
    "    load_best_model_at_end=True,  # Загружать лучшую модель после обучения\n",
    "    metric_for_best_model=\"eval_f1_macro\",  # Выбираем лучшую модель по eval_f1_macro\n",
    "    greater_is_better=True,  # Чем меньше eval_loss, тем лучше модель\n",
    "\n",
    "    # 🔴 Поддержка возобновления обучения\n",
    "    # save_steps=500,  # Сохранять каждые 500 шагов (на случай долгих эпох)\n",
    "    # resume_from_checkpoint=True,  # Автоматически продолжать обучение с последнего чекпоинта\n",
    "    # trainer.train(resume_from_checkpoint=\"./saved_model/checkpoint-1500\")\n",
    "\n",
    "    # 🟡 Параметры обучения (batch_size, precision, градиентное накопление)\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Размер батча на одно устройство (GPU/CPU)\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Размер батча для валидации\n",
    "    # gradient_accumulation_steps=4,  # Градиентное накопление (симулирует batch_size в 4 раза больше)\n",
    "    # fp16=True,  # Включить mixed precision (ускоряет обучение на GPU)\n",
    "\n",
    "    # # 🔵 Оптимизация скорости (ускорение загрузки данных)\n",
    "    # group_by_length=True,  # Динамическое изменение batch_size (ускоряет обучение)\n",
    "    # dataloader_num_workers=4,  # Количество потоков для загрузки данных\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "\n",
    "steps_per_epoch = len(train_dataset.df) // training_args.per_device_train_batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.2 * total_steps),    # 20% шагов на разогрев\n",
    "    num_training_steps=total_steps              # полный цикл косинусного затухания\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomBaseTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=train_dataset.tokenizer,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acce5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33452675",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25824222",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.predict(test_dataset)\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b28b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(DATA_PATH_SAVE_MODELS / MODEL_NAME_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a303d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "AutoModelForSequenceClassification.from_pretrained(DATA_PATH_SAVE_MODELS / MODEL_NAME_SAVE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
