{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce5d94a",
   "metadata": {},
   "source": [
    "# X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ebf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def seed_all(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.reset_peak_memory_stats()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('../../../data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_DOWNLOAD = DATA_PATH / Path('download/')\n",
    "DATA_PATH_DOWNLOAD.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_DATASET = DATA_PATH / Path('datasets/')\n",
    "DATA_PATH_DATASET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETICS = DATA_PATH / Path('synthetics/')\n",
    "DATA_PATH_DATASET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = DATA_PATH / Path('cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = DATA_PATH / Path('models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_IMGS = DATA_PATH /  Path('imgs/')\n",
    "DATA_IMGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = 'cointegrated/rubert-tiny2'\n",
    "# BASE_MODEL_NAME = 'DeepPavlov/rubert-base-cased'\n",
    "MODEL_NAME_SAVE = \"ner_x5\"\n",
    "MODEL_CHECKPOINT_PATH = \"ner_x5_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66775d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "DATA_LOG = DATA_PATH / Path(f'../logs/{MODEL_NAME_SAVE}_{current_date}')\n",
    "DATA_LOG.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 8\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008b155",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db66fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5 = pd.read_csv(DATA_PATH_DATASET / \"train.csv\", sep=\";\")\n",
    "\n",
    "df_x5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19774fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_annotations(ann_list):\n",
    "    if isinstance(ann_list, str):\n",
    "        ann_list = eval(ann_list)\n",
    "    new_list = []\n",
    "    for start, end, label in ann_list:\n",
    "        if label == \"0\":\n",
    "            label = \"O\"\n",
    "        new_list.append((start, end, label))\n",
    "    return new_list\n",
    "\n",
    "df_x5[\"annotation\"] = df_x5[\"annotation\"].apply(normalize_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0514259",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "\n",
    "for ann_list in df_x5[\"annotation\"]:\n",
    "    if isinstance(ann_list, str):\n",
    "        ann_list = eval(ann_list)\n",
    "    for _, _, label in ann_list:\n",
    "        all_labels.add(label)\n",
    "\n",
    "unique_labels = sorted(all_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c13579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(DATA_PATH_DATASET / \"submission.csv\", sep=\";\")\n",
    "df_submission[\"annotation\"] = df_submission[\"annotation\"].apply(normalize_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c18957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "def load_synthetic_file(file_path: str | Path) -> pd.DataFrame:\n",
    "    file_path = Path(file_path)\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                text, ann_str = line.split(\";\", 1)\n",
    "                text = text.strip()\n",
    "                annotation = ast.literal_eval(ann_str.strip())\n",
    "                data.append({\"sample\": text, \"annotation\": annotation})\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке строки в {file_path}: {line}\\n{e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_dataset(data_dir: str | Path) -> pd.DataFrame:\n",
    "    data_dir = Path(data_dir)\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in data_dir.glob(\"*.txt\"):\n",
    "        df_file = load_synthetic_file(file_path)\n",
    "        all_dfs.append(df_file)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"sample\", \"annotation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6838138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic = load_synthetic_dataset(DATA_SYNTHETICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce666f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x5[\"df_label\"] = \"x5\"\n",
    "df_synthetic[\"df_label\"] = \"synthetic\"\n",
    "\n",
    "dfs = []\n",
    "dfs.append(df_x5)\n",
    "dfs.append(df_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat(dfs, ignore_index=True)\n",
    "df_train = df_train.drop_duplicates(subset=[\"sample\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e3993",
   "metadata": {},
   "source": [
    "# Словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c21ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = {label for anns in df_train[\"annotation\"] for _, _, label in anns if label != \"O\"}\n",
    "\n",
    "b_labels = sorted([lbl for lbl in unique_labels if lbl.startswith(\"B-\")])\n",
    "i_labels = {lbl[2:]: lbl for lbl in unique_labels if lbl.startswith(\"I-\")}\n",
    "\n",
    "all_labels = [\"O\"]\n",
    "for b in b_labels:\n",
    "    all_labels.append(b)\n",
    "    base = b[2:]\n",
    "    if base in i_labels:\n",
    "        all_labels.append(i_labels[base])\n",
    "\n",
    "label2idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "print(\"label2idx:\", label2idx)\n",
    "print(\"idx2label:\", idx2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c72237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "label2idx_path = DATA_PATH_SAVE_MODELS / \"label2idx.json\"\n",
    "idx2label_path = DATA_PATH_SAVE_MODELS / \"idx2label.json\"\n",
    "\n",
    "with open(label2idx_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(idx2label_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2label, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Словарь label2idx сохранён в {label2idx_path}\")\n",
    "print(f\"Словарь idx2label сохранён в {idx2label_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d0c1a",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc707e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data: pd.DataFrame\n",
    "val_data: pd.DataFrame\n",
    "train_data, val_data = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.dataset import NerDataSet\n",
    "\n",
    "dtype_input = torch.long\n",
    "dtype_labels = torch.long\n",
    "\n",
    "train_dataset = NerDataSet(\n",
    "    df=train_data, \n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True    \n",
    ")\n",
    "\n",
    "val_dataset = NerDataSet(\n",
    "    df=val_data,\n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "test_dataset = NerDataSet(\n",
    "    df=df_submission, \n",
    "    max_length=MAX_LENGTH, \n",
    "    tokenizer_path=BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    label2idx=label2idx,\n",
    "    text_label='sample',\n",
    "    target_label='annotation',\n",
    "    dtype_input_ids=dtype_input,\n",
    "    dtype_token_type_ids=dtype_input,\n",
    "    dtype_attention_mask=dtype_input,\n",
    "    dtype_labels=dtype_labels,\n",
    "    debug=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ff487",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d39e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.plot_token_length_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5b27d",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9dc2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from ml.model import BertForTokenClassificationCRF, TokenClassifierCRFOutput\n",
    "\n",
    "num_labels = len(label2idx)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=idx2label,\n",
    "    label2id=label2idx,\n",
    "    cache_dir=DATA_CACHE,\n",
    ")\n",
    "\n",
    "model = BertForTokenClassificationCRF.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236b230",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec50deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DATA_PATH_SAVE_MODELS / MODEL_CHECKPOINT_PATH,  # Папка для сохранения моделей\n",
    "    num_train_epochs=8,  # Количество эпох\n",
    "    # learning_rate=3e-5,\n",
    "    # max_grad_norm=0.5,\n",
    "\n",
    "    # eval_strategy=\"epoch\",  # Оценка модели после каждой эпохи\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,  # Оценка модели каждый шаг\n",
    "    # logging_strategy=\"epoch\",  # Логирование каждые N шагов\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,  # Как часто логировать\n",
    "    disable_tqdm=False,  # Отключить tqdm (нужно для работы в Colab/Kaggle)\n",
    "    report_to=\"tensorboard\",  # Логируем в TensorBoard\n",
    "    logging_dir=DATA_LOG,  # Папка для логов\n",
    "\n",
    "    # save_strategy=\"epoch\",  # Сохранение модели после каждой эпохи\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25, # Если save_strategy=\"steps\"\n",
    "    save_total_limit=5,  # Храним все\n",
    "    load_best_model_at_end=True,  # Загружать лучшую модель после обучения\n",
    "    metric_for_best_model=\"eval_f1_macro\",  # Выбираем лучшую модель по eval_f1_macro\n",
    "    greater_is_better=True,  # Чем меньше eval_loss, тем лучше модель\n",
    "\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Размер батча на одно устройство (GPU/CPU)\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Размер батча для валидации\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-6)\n",
    "\n",
    "steps_per_epoch = len(train_dataset.df) // training_args.per_device_train_batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),    # 20% шагов на разогрев\n",
    "    num_training_steps=total_steps              # полный цикл косинусного затухания\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d590b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def compute_class_weights(train_dataset, num_labels, max_weight: float = 10.0, smoothing: float = 1.0):\n",
    "    all_labels = []\n",
    "    for labels in train_dataset.labels.tolist():\n",
    "        for l in labels:\n",
    "            if l != -100:\n",
    "                all_labels.append(l)\n",
    "    counts = Counter(all_labels)\n",
    "    total = sum(counts.values())\n",
    "    class_weights = [total / (counts.get(i, 0) + smoothing) for i in range(num_labels)]\n",
    "    class_weights = np.array(class_weights, dtype=np.float32)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    class_weights = np.clip(class_weights, 0.0, max_weight)\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def weighted_crf_loss(\n",
    "    emissions,\n",
    "    labels,\n",
    "    attention_mask,\n",
    "    crf,\n",
    "    class_weights_tensor,\n",
    "    alpha=0.7,\n",
    "    verbose=True,\n",
    "):\n",
    "    labels_for_crf = labels.clone().long()\n",
    "    labels_for_crf[labels_for_crf == -100] = 0\n",
    "\n",
    "    crf_loss = -crf(\n",
    "        emissions,\n",
    "        labels_for_crf,\n",
    "        mask=attention_mask.bool(),\n",
    "        reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "    logits_flat = emissions.view(-1, emissions.size(-1))\n",
    "    labels_flat = labels.view(-1)\n",
    "    ce_loss_fn = CrossEntropyLoss(\n",
    "        weight=class_weights_tensor.to(emissions.device),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    ce_loss = ce_loss_fn(logits_flat, labels_flat)\n",
    "\n",
    "    total_loss = alpha * crf_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "\n",
    "    # print(f\"[DEBUG] crf_loss={crf_loss.item():.4f}, ce_loss={ce_loss.item():.4f}, total={total_loss.item():.4f}\")\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def weighted_loss(\n",
    "    outputs: \"TokenClassifierCRFOutput\",\n",
    "    labels,\n",
    "    model,\n",
    "    class_weights_tensor,\n",
    "    alpha=0.7,\n",
    "    num_items_in_batch=None,\n",
    "):\n",
    "    emissions = outputs.logits\n",
    "    loss = weighted_crf_loss(\n",
    "        emissions=emissions,\n",
    "        labels=labels,\n",
    "        attention_mask=outputs.attention_mask,\n",
    "        crf=model.crf,\n",
    "        class_weights_tensor=class_weights_tensor,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_tensor = compute_class_weights(train_dataset, num_labels, max_weight=10.0, smoothing=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from ml.trainer import CustomBaseTrainer\n",
    "from ml.metrics import compute_metrics\n",
    "\n",
    "trainer = CustomBaseTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=partial(compute_metrics, idx2label=idx2label),\n",
    "    processing_class=train_dataset.tokenizer,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_loss_func=partial(weighted_loss, model=model, class_weights_tensor=class_weights_tensor, alpha=0.9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33452675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25824222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = trainer.predict(test_dataset)\n",
    "# print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b28b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(DATA_PATH_SAVE_MODELS / MODEL_NAME_SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6c60b",
   "metadata": {},
   "source": [
    "# Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40422331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regex import D\n",
    "from transformers import AutoConfig\n",
    "from ml.model import BertForTokenClassificationCRF, TokenClassifierCRFOutput\n",
    "\n",
    "num_labels = len(label2idx)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    DATA_PATH_SAVE_MODELS / \"ner_x5_next\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=idx2label,\n",
    "    label2id=label2idx,\n",
    "    cache_dir=DATA_CACHE,\n",
    ")\n",
    "\n",
    "model = BertForTokenClassificationCRF.from_pretrained(\n",
    "    DATA_PATH_SAVE_MODELS / \"ner_x5_next\",\n",
    "    cache_dir=DATA_CACHE,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79161a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomBaseTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=partial(compute_metrics, idx2label=idx2label),\n",
    "    processing_class=train_dataset.tokenizer,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_loss_func=partial(weighted_loss, model=model, class_weights_tensor=class_weights_tensor, alpha=0.7)\n",
    ")\n",
    "\n",
    "test_results = trainer.predict(test_dataset)\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.analyze_with_model(\n",
    "    model=model, \n",
    "    idx2label=idx2label, \n",
    "    batch_size=64, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "    layer=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bf962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset.df[\"is_correct\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = test_dataset.df[test_dataset.df[\"is_correct\"] == True]\n",
    "df_true.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_false = test_dataset.df[test_dataset.df[\"is_correct\"] == False]\n",
    "df_false.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f93572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.visualize_embeddings(\n",
    "#     source=\"mean\",\n",
    "#     method='tsne', \n",
    "#     n_components=2,\n",
    "#     n_samples=1000,\n",
    "#     # cluster_method='kmeans',\n",
    "#     # n_clusters=10,\n",
    "#     # use_clusters=True,\n",
    "#     use_opacity=False,\n",
    "#     idx2label=idx2label,\n",
    "#     colorscale_name='rainbow',\n",
    "#     hover_columns=['sample', 'annotation'],\n",
    "#     plot_width=1800,\n",
    "#     plot_height=600,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d6a98",
   "metadata": {},
   "source": [
    "# Ручное тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a303d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from ml.pipline import NERPipelineCRF\n",
    "\n",
    "model_dir = DATA_PATH_SAVE_MODELS / \"ner_x5_88\"\n",
    "label2idx_path = DATA_PATH_SAVE_MODELS / \"label2idx.json\"\n",
    "idx2label_path = DATA_PATH_SAVE_MODELS / \"idx2label.json\"\n",
    "\n",
    "with open(label2idx_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "\n",
    "with open(idx2label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    idx2label = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "pipeline = NERPipelineCRF(\n",
    "    model_path=model_dir,\n",
    "    label2idx=label2idx,\n",
    "    idx2label=idx2label,\n",
    "    max_length=16\n",
    ")\n",
    "\n",
    "text = [\n",
    "    \"йогурт данисимо фантазия\",\n",
    "    \"молоко простоквашино 3.2% 930г\",\n",
    "    \"хлеб бородинский нарезка 300г\",\n",
    "    \"мороженнае как бы его взтять\",\n",
    "    \"молоко⁷\",\n",
    "    \"погремушки fisher-pri\"\n",
    "]\n",
    "entities = pipeline.predict(text)\n",
    "\n",
    "for i, entity in enumerate(entities):\n",
    "    print(f\"Текст: {text[i]}\")\n",
    "    print(entity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
